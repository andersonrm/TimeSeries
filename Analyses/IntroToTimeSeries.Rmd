---
title: "Intro to Time Series"
author: "Riley M. Anderson"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
graphics: yes
output:
  github_document:
    toc: yes
    toc_depth: 5
    pandoc_args: --webtex
  html_document:
    keep_md: yes
    theme: readable
    mathjax: default
  html_notebook:
    code_folding: hide
    theme: readable
    mathjax: default
  pdf_document:
    toc: yes
header-includes:
  \usepackage{float}
  \floatplacement{figure}{H}
editor_options:
  chunk_output_type: console
---

```{r setup, include = F}
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# @@@@@ Knitr Options
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

# Set root directory to the project directory
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())


# Set default knitr options: 
# Suppress warnings and messages, cache chunks, 
#  set default figure size to 6x8 at 300 dpi, and save a png and pdf
knitr::opts_chunk$set(warning = F, message = F, collapse = T, cache = T,
    fig.height = 6, fig.width = 8, dpi = 300, # 6x8" @ 300dpi:1800x2400=4.3MP
    dev = c('png', 'pdf'), dev.args = list(pdf = list(onefile = F)))

```




```{r Main_Code, include = F, cache = F}

# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# @@@@@ Setup - This code is run, but output is hidden
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

# Load Packages
library(tidyverse) # Needed for data wrangling: dplyr, tidyr, ggplot2
library(cowplot) # Needed for publication-quality ggplots
library(IRdisplay)
library(magrittr)
library(tidyverse)
library(scales)
library(gridExtra)
library(fpp2)
library(tseries)
library(ggthemes)
theme_set(theme_classic())

# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# @@@@@ Data Preparation
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

# Import datasets

ur <- read.csv("Data/Mass Monthly Unemployment Rate.csv")


########################################
# Functions
########################################

source("R Functions/compare_models_function.R")
source("R Functions/sim_random_walk_function.R")
source("R Functions/sim_stationary_example_function.R")

```

# Time Series Analysis

(Univariate) time series data is defined as sequence data over time:  $X_{1}$, $X_{2}$,..., $X_{T}$
 
where  *T*  is the time period and  $X_{t}$ is the value of the time series at a particular point.

Examples: daily temperatures in Boston, US presidential election turnout by year, minute stock prices

Variables in time series models generally fall into three categories:

(1) endogenous (past values of the time-series in questions)

(2) random noise (accounting for uncertainty (CIs), and past random shocks)

(3) exogenous (predictors outside the time-series)

All time series models involve (1) and (2) but (3) is optional.



## Why use time-series?

(1) many forecasting tasks actually involve small samples which makes machine learning less effective

(2) time series models are more interpretable and less black box than machine learning algorithms

(2) time series appropriately accounts for forecasting uncertainty.

As an example, lets look at the following data generating process known as a random walk:  $X_{t} = X_{t-1} + \varepsilon_{t}$
 
We can compare the forecasting performance of linear regression to that of a basic time series model known as an AR(1) model.


```{r compare_models, echo = F}

compare.models(n=100)

```

Linear regression overfits the past observations and doesn't account for compounding error. With successive future predictions, the confidence interval should increase over time. Too much weight on random error.

AR model just forecasts the average $X$ values from the past observations.



## Autocorrelation & Autocovariance

Autocorrelation/autocovariance refers to the correlation/covariance between two observations in the time series at different points.

The central idea behind it is how related the data/time series is over time.

For ease of interpretation we typically focus on autocorrelation i.e. what is the correlation between $X_{t}$ and  $X_{t + p}$ for some integer *p*. (Correlation between all *p* periods about in the sample)

A related concept is partial autocorrelation that computes the correlation adjusting for previous lags/periods i.e. the autocorrelation between $X_{t}$ and  $X_{t + p}$ adjusting for the correlation of $X_{t}$ and  $X_{t + 1}$, ... , $X_{t + p - 1}$.

When analyzing time series we usually view autocorrelation/partial autocorrelation in ACF/PACF plots.

Let's view this for the random walk model we analyzed above: $X_{t} = X_{t-1} + \varepsilon_{t}$

```{r simulate_random_walk, echo = T}
dat <- sim.random.walk(n = 100)

#plot random walk
sim.plot <- dat %>% ggplot(aes(t,X)) +
  geom_line() +
  xlab("T") +
  ylab("X") +
  ggtitle("Time Series Plot")

#ACF plot
sim.corr <- ggAcf(dat$X,type="correlation") +
  ggtitle("Autocorrelation ACF Plot")

#PACF plot
sim.partial <- ggAcf(dat$X,type="partial") +
  ggtitle("Partial Autocorrelation PACF Plot")

grid.arrange(sim.plot, sim.corr, sim.partial)

```

The ACF plot shows a high degree of correlation between each successive lag. This makes sense because we simulated this data with a random walk, where each successive time point value $X_{t}$ is a function of the previous time point value $X_{t-1}$.

In contrast, the PACF plot shows the correlation between lags $p_{1},...,n$. The correlation is high when the lag is 1, but is weak for lags > 1 (i.e., lags with greater distance in time points).




# Stationarity vs Nonstationarity

Is the distribution of the data over time consistent?

There are two main forms of stationarity.

* Strict stationarity imples:

  + The cumulative distribution function of the data does not depend on time:

$F_{X}(X_{1},...,X_{T}) = F_{X}(X_{1 + \Delta},...,X_{T + \Delta})  \forall \Delta \in  \mathbb{R}$
 

* Weak stationarity implies:

  + the mean of the time series is constant
$E(X_{T}) = E(X_{t + \Delta})$
 
  + the autocovariance/autocorrelation only depends on the time difference between points
$ACF(X_{t}, X_{t + \Delta - 1}) = ACF(X_{1}, X_{\Delta})$
 
  + the time series has a finite variance
$Var(X_{\Delta}) < \infty  \forall \Delta \in  \mathbb{R}$

```{r station_v_nonstation, echo = T}

df<-sim.stationary.example(n=1000)

head(df)

gns <- ggplot(df, aes(x = t, y = X1)) +
  geom_line() +
  labs(x = "T", y = "X1", title = "Nonstationary")

gs <- ggplot(df, aes(x = t, y = X3)) +
  geom_line() +
  labs(x = "T", y = "X3", title = "Stationary")

grid.arrange(gns, gs)

```

The nonstationary time series has an unstable mean, that changes over time. Whereas the stationary time series appears to have a constant mean over time, and finite variance.


```{r ACF_plots_S_vs_NS, echo = T}

#ACF for nonstationary and stationary time series
nsACF <- ggAcf(df$X1, type = "correlation") +
  labs(x = "T", y = "X1", title = "Nonstationary")

sACF <- ggAcf(df$X3, type = "correlation") +
  labs(x = "T", y = "X3", title = "Stationary")

grid.arrange(nsACF, sACF)

```

The nonstationary time series has all significant correlations across lags and shows non mean-reverting behavior.

The stationary time series has non-significant correlations between almost all lags.

We can perform a unit root test (Augmented Dickey-Fuller test) to test for stationarity:
```{r ADF_tests_nonstationary, echo = T}

adf.test(df$X1)

```

In the ADF test, the null hypothesis is *nonstationary*, while the alternative hypothesis is *stationary*. For the above time series, we fail to reject to null hypothesis (*P* = `r round(adf.test(df$X1)[[4]], 3)`) and conclude that the time series exhibits nonstationarity.

```{r ADF_tests_stationary, echo = T}

adf.test(df$X3)

```

In the stationary example, the ADF test confirms stationarity by rejecting the null hypothesis (*P* = `r round(adf.test(df$X3)[[4]],3)`).


## Transforming for stationarity

Typically, nonstationary data are transformed by differencing or detrending to make the data more stationary.


### Differencing

Differencing involves taking differences between successive time series values.

The order of differencing is defined as *p* for $X_{t} - X_{t-p}$.

Let's transform a nonstationary time series to stationary by differencing with the random walk model.

In a random walk $X_{t} = X_{t-1} + \varepsilon_{t}$ where $\varepsilon_{t} \sim N(0,\sigma^{2}) iid$.

Differencing with an order of one means that $\tilde{X}_{t} = X_{t}-X_{t-1} = \varepsilon_{t}$.

With data generated by random walk, the differencing order is t-1, which if we difference by this order, we have removed the $X_{t}-X_{t-1}$ term and are left with the random noise term $\varepsilon_{t}$, which is by definition, stationary.


```{r differencing_in_time_series, echo = T}

# differencing with an order of 1
diff <- df$X1 - lag(df$X1, 1) # for all values of X1, subtract the previous (1th) value


#plot original and differenced time series
untrans <- ggAcf(df$X1, type = "correlation")

lag.trans <- ggAcf(diff, type = "correlation")

grid.arrange(untrans, lag.trans)
```

The ACF for df$X1 (generated by random walk), does not die off quickly, suggesting high correlations between each successive term (lag).

However, the lag transformed (1 lag difference between time points) has weak correlations between these lags, showing stationarity.



### Detrending

An alternative way to induce stationarity from nonstationary data.


Detrending involves removing a deterministic relationship with time.

As an example suppose we have the following data generating process  $X_{t} = B_{t} + \varepsilon_{t}$ where $\varepsilon_{t} = N(0,\sigma^{2})iid$.

Here, we have a deterministic linear relationship with time, and removing that relationship would leave the transformed time series $\tilde{X}_{t}$ where the only predictor is the random noise term $\varepsilon_{t}$, which is by definition stationary with constant mean and finite variance.

Detrending involves using the transformed time series  $\tilde{X}_{t} = X_{t}-B_{t} = \varepsilon_{t}$.

In the real world, we don't know the actual relationship with time, so we build a model and then remove the time component ($B_{t}$) leaving only the random noise term (residual error $\varepsilon_{t}$).

```{r detrending_with_linear_regression, echo = T}

detrended <- resid(lm(X2 ~ t, data = df))

#plot original and detrended time series
untransX2 <- ggAcf(df$X2, type = "correlation")

detrendedX2 <- ggAcf(detrended, type = "correlation")

grid.arrange(untransX2, detrendedX2)

```

The original data exhibit nonstationary behavior, with correlations extending far beyond the confidence intervals (highly significant).

Alternatively, the detrended time series correlations fall mainly within the confidence intervals (not significant) and exhibit stationary behavior.



# Basic Model Types: AR(p), MA(q), ARMA(p,q), ARIMA(p,d,q), Decomposition

## Autoregressive AR(p) Models
AR models specify $X_{t}$ as a function of lagged time series values $X_{t-1}, X_{t-2},...$ i.e $X_{t} = \mu + \phi_{1}X_{t-p}+...+\phi_{p}X_{t-p}+\varepsilon_{t}$
 
where  $\mu$ is a mean term and $\varepsilon_{t} \sim N(0,\sigma^{2})$ is a random error. $X_{t}$ is a a function of the $\mu$ (mean/intercept) term plus a linear combination of past lagged values. Our model will estimate the $\phi_{i}$ parameters using one of several methods (Yule-Walker equations, Maximum Likelihood, or even linear regression).

AR models can sometimes be non-stationary if a unit root is present (rejected the null in an ADF test).

In AR models, because we depend on past lagged $X_{i}$ values, random noise terms in past periods can have influence on the future predictions. Conceptually, this is why the process can be nonstationary. Past large shocks can cause a series to diverge and become non mean-reverting and have infinite variance.

When fitting an AR model the key choice is *p*, the number of lags (and $\phi$ parameters) to include in the model.

## Moving Average MA(q) Models

Similar to AR models, but instead of using past lagged values of the time series, we use past lagged random shock events (random error terms)

MA models specify $X_{t}$ using random noise lags:

$X_{t} = \mu + \varepsilon_{t} + \Theta_{1}\varepsilon_{t-1}+...+\Theta_{q}\varepsilon_{t-q}$
 
where $\mu$ is a mean term and $\varepsilon_{t} \sim N(0,\sigma^{2})$ is a random error. $X_{t}$ is a a function of the $\mu$ (mean/intercept) term plus the random noise term in the current period $\varepsilon_{t}$ plus a linear combination of the past random noise terms $\Theta_{1,...,p}$. Our model will estimate the $\phi_{i}$ parameters using one of several methods

Because MA models specify $X_{t}$ as a function of past random noise terms, we don't actually observe these past random noise terms, fitting them is more complicated and involves interative fitting. Also, because MA models are a linear combination of past random shocks, they are by construct, stationary (random noise terms are independent and identically distributed, *iid*, constant mean and finite variance).

$X_{t}$ is a function of a finite number of random lags, past random shocks can only have a finite influence on future periods. This is why MA models are always stationary, whatever influence the random shocks have will trend toward zero over time.

Similar to an AR model, when fitting an MA model the key choice is *q*, the number of random shock lags.

## Autoregressive Moving Average ARMA(p,q) Models

ARMA(p,q) models are a combination of an AR and MA model:

$X_{t} = \mu + \phi_{1}X_{t-1}+...+\phi_{p}X_{t-p} + \varepsilon_{t} + \Theta_{1}\varepsilon_{t-1} +...+\Theta_{q}\varepsilon_{t-q}$
 
where $\mu$ is a mean term and $\varepsilon_{t} \sim N(0,\sigma^{2})$ is a random error. $X_{t}$ is a function of a $\mu$ (mean/intercept) term plus a random noise term in the current period $\varepsilon_{t}$ plus a linear combination of both past lagged time series values and also past random noise terms

When fitting an ARMA model, we need to choose two things: p, the number of AR lags, and q, the number of MA lags.

## Autoregressive Integrated Moving Average ARIMA(p,d,q) Models
ARIMA(p,d,q) is an ARMA model with differencing.

Take an ARMA model and add in differencing. We take the difference between the time series at successive points in time.

When fitting an ARIMA model we need to choose three things: p, the number of AR lags, q, the number of MA lags, and d, the number of differences to use.

## Decomposition Models

Decomposition models specify $X_{t}$ as a combination of a trend component ($T_{t}$), seasonal component ($S_{t}$), and an error component/residual ($E_{t}$) i.e.  $X_{t} = f(T_{t},S_{t},E_{t})$.

Common decomposition forms are: $X_{t} = T_{t}+S_{t}+E_{t}$ or $X_{t} = T_{t}*S_{t}*E_{t}$ (where we then take logs to recover the additive form).

There are various ways to estimate the different trend components: exponential smoothing, state space models/Kalman filtering, STL models, etc.



# Fitting AR/MA/ARMA/ARIMA models with the Box Jenkins Method

How to fit AR/MA/ARMA/ARIMA models on a real data set and review a generic strategy for fitting them (Box Jenkins method).

This process involves several steps to help identify the *p*, *d*, and *q* parameters that we need:

* Identify whether the time series is stationary or not

* Identify *p*, *d*, and *q* of the time series by

  + Making the time series stationary through differencing/detrending to find *d*
  + Looking at ACF/PACF to find *p* and *q*
  + Using model fit diagnostics like AIC or BIC to select the best model to find *p*, *d*, and *q*

* Check the model fit using the Ljung-Box test


## Fit some time series with real data

This data comes from the FRED database (St. Louis Federal Reserve) and describes the monthly unemployment rate for Massachusetts between January 1976 and January 2020.
```{r unemployment data, echo = F, comment = ""}

ur <- ur %>% 
  mutate(DATE = as.Date(DATE))

head(ur)

```

Where MAURN is the monthly unemployment rate.

### Check for stationarity
```{r ur_stationarity_plot, echo = T}

ur %>% 
  ggplot(aes(x = DATE, y = MAURN)) +
  geom_line()

```

Doesn't look stationary, mean varies over time.

```{r ur_stationarity_ACF, echo = T}

ggAcf(ur$MAURN, type = "correlation")

```
Again, looks nonstationary.

```{r ur_stationarity_ADF, echo = T}

adf.test(ur$MAURN)

```
Fail to reject the null hypothesis of non-stationarity.

## Transforming for Stationarity & Identifying Model Parameters
```{r AR_model, echo = T}

ar.mod <- auto.arima(ur$MAURN,
                     max.d = 0, # no differencing
                     max.q = 0, # no random noise terms
                     allowdrift = T #include a mu term
                     # note only p is allowed to vary -> AR model
                     # p is the number of autoregressive lags
                     )

ar.mod

```

```{r MA_model, echo = T}

ma.mod <- auto.arima(ur$MAURN,
                     max.d = 0, # no differencing
                     max.p = 0, # no lags
                     allowdrift = T #include a mu term
                     # note only q is allowed to vary -> MA model
                     # q is the number of random noise (shock lag) events
                     )

ma.mod

```

```{r ARMA_model, echo = T}

arma.mod <- auto.arima(ur$MAURN,
                     max.d = 0, # no differencing
                     allowdrift = T #include a mu term
                     # note p and q are allowed to vary -> ARMA model
                     # p is the number of autoregressive lags
                     # q is the number of random noise (shock lag) events
                     )

arma.mod

```

```{r ARIMA_model, echo = T}

arima.mod <- auto.arima(ur$MAURN,
                     allowdrift = T #include a mu term
                     # note d, p, and q are allowed to vary -> ARIMA model
                     # d is the 
                     # p is the number of autoregressive lags
                     # q is the number of random noise (shock lag) events
                     )

arima.mod

```


## Checking the Residuals of the Model Fit (Ljung-Box test)
```{r ljung_box, echo = T}

#calculate residuals of each model
ar.resid <- resid(ar.mod)
ma.resid <- resid(ma.mod)
arma.resid <- resid(arma.mod)
arima.resid <- resid(arima.mod)

```

```{r ar_resid_plot, echo = T}

#plot PACF plot of each models residuals
ggAcf(ar.resid, type = "partial")

Box.test(ar.resid, type = "Ljung-Box", lag = 1)

```

```{r ma_resid_plot, echo = T}

#plot PACF plot of each models residuals
ggAcf(ma.resid, type = "partial")

Box.test(ma.resid, type = "Ljung-Box", lag = 1)
```

```{r arma_resid_plot, echo = T}

#plot PACF plot of each models residuals
ggAcf(arma.resid, type = "partial")

Box.test(arma.resid, type = "Ljung-Box", lag = 1)
```

```{r arima_resid_plot, echo = T}

#plot PACF plot of each models residuals
ggAcf(arima.resid, type = "partial")

Box.test(arima.resid, type = "Ljung-Box", lag = 1)
```


## Making a forecast for each model

```{r forecasts_arima, echo = T}

#make forecast for each model
ar.fc <- forecast(ar.mod, h = 24, level = 80)
ma.fc <- forecast(ma.mod, h = 24, level = 80)
arma.fc <- forecast(arma.mod, h = 24, level = 80)
arima.fc <- forecast(arima.mod, h = 24, level = 80)


#plot forecast for each model
g1 <- autoplot(ar.fc)
g2 <- autoplot(ma.fc)
g3 <- autoplot(arma.fc)
g4 <- autoplot(arima.fc)
grid.arrange(g1, g2, g3, g4)

```

# Fitting Seasonal Trend Loess (STL) Decomposition Models

```{r STL_model, echo = T}

#transform to time series object; need to specify frequency
ur.ts <- ts(ur$MAURN, frequency = 12) # monthly data (12 months/year)

#fit stil model
stl.mod <- stl(ur.ts, s.window = "periodic")


#plot model fit
autoplot(stl.mod)
```


```{r STL_forecast, echo = T}
#make forecast
stl.fc <- forecast(stl.mod, h = 24, level = 80)

autoplot(stl.fc)

```

# Exponential Smoothing

* From https://www.geeksforgeeks.org/exponential-smoothing-in-r-programming/#

Exponential smoothing is the oldest version of time series analysis and is most useful when we want more recent events to have a greater influence on our forecasts than more distant events (i.e., inverse time weighting). 

This model works by assigning exponentially decreasing weights to events further in the past. Additionally, the "smoothing" component acts as a low-pass filter, generally smoothing out the trend over time by removing high-frequency noise.

Basic exponential smoothing models include:

* Simple Exponential Smoothing
* Holt's method
* Holt-Winter's Seasonal method
* Damped Trend method

# Simple Exponential Smoothing

* Used for data that has no trend or seasonal pattern

Weight of every parameter is determined by a smoothing parameter, $\alpha$ with range 0-1, but in practice is usually between 0.1 and 0.2. When $\alpha$ is closer to 0, historical events will be weighted similar to recent events ("slow-learning"). $\alpha$ close to 1 means extreme weighting on recent event where historical values have little influence on future predictions ("fast-learning").


An example using Google's stock price data
```{r SES_google, echo = T}

# training data:
google.train <- window(goog, end = 900)

# test data:
google.test <- window(goog, start = 901)

# Performing SES on  the 
# Google stock data
ses.goog <- ses(google.train, 
                alpha = .2,
                h = 100) # h is number of time points into the future
autoplot(ses.goog)

```

The flat line forecast suggests that the model is not capturing the present trend.

We can remove the trend with the diff() function.

```{r SES_google_diff, echo = T}

google.diff <- diff(google.train)

#autoplot(google.diff)

ses.goog.diff <- ses(google.diff,
                     alpha = 0.2,
                     h = 100)

autoplot(ses.goog.diff)

```

## Validation of SES

Create a differenced validation data set to compare with the differenced training set. Then we'll iterate the model fit through all values $\alpha$ 0.01 - 0.99 to find the optimal value that minimizes RMSE.

```{r ses_validation, echo = T}

# remove the trend from the test data
goog.diff.test <- diff(google.test)

accuracy(ses.goog.diff, goog.diff.test)[2,2]

# comparing our model
alpha <- seq(.01, .99, by = .01)
RMSE <- NA
for(i in seq_along(alpha)) {
  fit <- ses(google.diff, alpha = alpha[i],
             h = 100)
  RMSE[i] <- accuracy(fit, 
                      goog.diff.test)[2,2]
}

# convert to a data frame and 
# identify min alpha value
alpha.fit <- tibble(alpha, RMSE)
alpha.min <- filter(alpha.fit, 
                    RMSE == min(RMSE))
 
# plot RMSE vs. alpha
ggplot(alpha.fit, aes(alpha, RMSE)) +
  geom_line() +
  geom_point(data = alpha.min,
             aes(alpha, RMSE), 
             size = 2, color = "red")


```


The minimum RMSE was reached at $\alpha$ = 0.01, which suggests increasing the weights of more historical values will better capture the trend of Google's stock price.

Refit the forecast to the optimized $\alpha$ value of 0.01.

```{r SES_google_optimized, echo = T}

# refit model with alpha = .01
ses.goog.opt <- ses(google.diff, 
                    alpha = .01,
                    h = 100)
 
# performance eval
accuracy(ses.goog.opt, goog.diff.test)
 
# plotting results
p1 <- autoplot(ses.goog.opt) +
  theme(legend.position = "bottom")
p2 <- autoplot(goog.diff.test) +
  autolayer(ses.goog.opt, alpha = .5) +
  ggtitle("Predicted vs. actuals for
                 the test data set")
 
grid.arrange(p1, p2, 
                        nrow = 1)

```

# Holt's method

With SES we had to remove the long-term trends to improve the model. But in **Holt’s Method**, we can apply exponential smoothing while we are capturing the time trend in the data. This is a technique that works with data having a time trend but no seasonality. In order to make predictions on the data, Holt’s Method uses **two smoothing parameters, alpha, and beta**, which correspond to the level components and trend components. 

```{r holt_google, echo = T}

# applying holt's method on
# Google stock Data
holt.goog <- holt(google.train,
                  h = 100)

autoplot(holt.goog)

```

Holt's method will automatically set the optimal value of $\alpha$, but we can optionally set the parameters as well. Again, $\alpha$ close to 1 indicates fast learning, while $\alpha$ close to 0 indicates slow learning.

```{r holt_google_accuracy, echo = T}

# holt's method
holt.goog$model
 
# accuracy of the model
accuracy(holt.goog, google.test)

```

The parameter $\beta$ = 0.0001 was used to remove errors from the training set. We can further tune our beta from this value. 

We can optimize the value of $\beta$ through a loop ranging from 0.0001 to 0.5 that will minimize the RMSE test. 

```{r holt_google_optimize_beta, echo = T}

# identify optimal beta parameter
beta <- seq(.0001, .5, by = .001)
RMSE <- NA
for(i in seq_along(beta)) {
  fit <- holt(google.train,
              beta = beta[i], 
              h = 100)
  RMSE[i] <- accuracy(fit, 
                      google.test)[2,2]
}
 
# convert to a data frame and
# identify min beta value
beta.fit <- data_frame(beta, RMSE)
beta.min <- filter(beta.fit, 
                   RMSE == min(RMSE))
 
# plot RMSE vs. beta
ggplot(beta.fit, aes(beta, RMSE)) +
  geom_line() +
  geom_point(data = beta.min, 
             aes(beta, RMSE), 
             size = 2, color = "red")

```

The optimal $\beta$ value is `r beta.min[[1]]`. Now refit the training model with our optimal $\alpha$ and $\beta$ parameters and compare to the original model.

```{r holt_google_compare_models, echo = T}

holt.goog <- holt(google.train,
                  h = 100)
 
# new model with optimal beta
holt.goog.opt <- holt(google.train,
                      h = 100,
                      beta = beta.min[[1]])
 
# accuracy of first model
accuracy(holt.goog, google.test)
 
# accuracy of new optimal model
accuracy(holt.goog.opt, google.test)
 
p1 <- autoplot(holt.goog) +
  ggtitle("Original Holt's Model") +
  coord_cartesian(ylim = c(400, 1000))
 
p2 <- autoplot(holt.goog.opt) +
  ggtitle("Optimal Holt's Model") +
  coord_cartesian(ylim = c(400, 1000))
 
grid.arrange(p1, p2,nrow = 1)

```

The optimized model is more conservative, and the confidence interval is more extreme.



# Holt-Winter's Seasonal method

Google's stock prices aren't likely to have a major seasonal component, but something like crop yield, or gas prices likely would have strong seasonal components.

The Holt-Winter’s Seasonal method is used for data with both seasonal patterns and time trends. This method can be implemented either by using an *additive structure* or by using a *multiplicative structure* depending on the data set. The additive structure or model is used when the seasonal pattern of data has the same magnitude or is consistent throughout, while the multiplicative structure or model is used if the magnitude of the seasonal pattern of the data increases over time. It uses three smoothing parameters,- $\alpha$, $\beta$, and $\gamma$.

An additive model linearly decreases the weights of historic events, while a multiplicative model exponentially decreases the weights of historical events.

Use the decompose() function to perform this kind of exponential smoothing. This  example uses the qcement dataset.


The below example uses the Quarterly Australian Portland Cement Production data.
```{r holtwinter_qcement, echo = T}

# create training and validation
# of the qcement data
qcement.train <- window(qcement, 
                        end = c(2012, 4))
qcement.test <- window(qcement,
                       start = c(2013, 1))
 
# applying holt-winters 
# method on qcement
autoplot(decompose(qcement))


```

Here, the seasonal component does not appear to increase with time, suggesting an *additive model* will provide a better fit.

To create an Additive Model that deals with error, trend, and seasonality, we are going to use the ets() function. Out of the 36 models, the ets() chooses the best additive model. For additive model, the model parameter of ets() will be ‘AAA’.

```{r ets_qcement, echo = T}

# applying ets
qcement.hw <- ets(qcement.train, model = "AAA") # define additive

autoplot(forecast(qcement.hw))

```

Assess the model and summarize the smoothing parameters. Check the residuals and find out the accuracy of our model.

```{r ets_qcement_resids_accuracy, echo = T}
 
# assessing our model
summary(qcement.hw)
checkresiduals(qcement.hw)

```

Residuals look good.


```{r ets_qcement_forceast, echo = T}
# forecast the next 5 quarters
qcement.f1 <- forecast(qcement.hw, h = 5)
 
# check accuracy
accuracy(qcement.f1, qcement.test)

```

Accuracy is pretty good (RMSE = `r accuracy(qcement.f1, qcement.test)[[2,2]]`).


Here is an example of how the Multiplicative model works using ets(). For that purpose, the model parameter of ets() will be ‘MAM’. However, we already know that the additive model will fit this time series more appropriately.

```{r etsMAM_qcement, echo = T}

# applying ets
qcement.hw2 <- ets(qcement.train, model = "MAM") # define multiplicative

checkresiduals(qcement.hw2)

```


## Optimize the Holt-Winter's additive model

Next we can optimize the $\gamma$ parameter in order to minimize the error rate. Then find the accuracy and also plot the predictive values. Since the seasonal  

```{r etsAAA_qcement_optimize, echo = T}

# forecast the next 5 quarters
qcement.f1 <- forecast(qcement.hw,
                       h = 5)
 
# check accuracy
accuracy(qcement.f1, qcement.test)
 
gamma <- seq(0.01, 0.85, 0.01)
RMSE <- NA
 
for(i in seq_along(gamma)) {
  hw.expo <- ets(qcement.train, 
                 "AAA", 
                 gamma = gamma[i])
  future <- forecast(hw.expo, 
                     h = 5)
  RMSE[i] = accuracy(future, 
                     qcement.test)[2,2]
}
 
error <- data_frame(gamma, RMSE)

minimum <- filter(error, 
                  RMSE == min(RMSE))

ggplot(error, aes(gamma, RMSE)) +
  geom_line() +
  geom_point(data = minimum, 
             color = "blue", size = 2) +
  ggtitle("gamma's impact on 
            forecast errors",
  subtitle = "gamma = 0.21 minimizes RMSE")
 
# previous model with additive error, trend and seasonality
accuracy(qcement.f1, qcement.test)

```

Optimizing $\gamma$ has brought the RMSE from `r accuracy(qcement.f1, qcement.test)[1,2]` down to `r accuracy(qcement.f1, qcement.test)[2,2]`.


```{r etsAAA_qcement_forecast_optimized, echo = F}

# new model with optimal gamma parameter
qcement.hw6 <- ets(qcement.train,
                   model = "AAA", 
                   gamma = 0.21)
qcement.f6 <- forecast(qcement.hw6, 
                       h = 5)
accuracy(qcement.f6, qcement.test)
 
# predicted values
qcement.f6
autoplot(qcement.f6)

```



# Damping model

The damping method uses the **damping coefficient** $\phi$ to estimate more conservatively the predicted trends. The value of phi lies between 0 and 1. If we believe that our additive and multiplicative model is going to be a flat line then chances are that it is damped. To understand the working principle of damping forecasting we will use the fpp2::ausair data set where we will create many models and try to have much more conservative trend lines,

```{r damping_model, echo = T}

# Damping model in R
 
# holt's linear (additive) model
fit1 <- ets(ausair, model = "ZAN",
            alpha = 0.8, beta = 0.2)
pred1 <- forecast(fit1, h = 5)
 
# holt's linear (additive) model
fit2 <- ets(ausair, model = "ZAN", 
            damped = TRUE, alpha = 0.8, 
            beta = 0.2, phi = 0.85)
pred2 <- forecast(fit2, h = 5)
 
# holt's exponential
# (multiplicative) model
fit3 <- ets(ausair, model = "ZMN",
            alpha = 0.8, beta = 0.2)
pred3 <- forecast(fit3, h = 5)
 
# holt's exponential 
# (multiplicative) model damped
fit4 <- ets(ausair, model = "ZMN", 
            damped = TRUE,
            alpha = 0.8, beta = 0.2,
            phi = 0.85)
pred4 <- forecast(fit4, h = 5)
 
autoplot(ausair) +
  autolayer(pred1$mean, 
            color = "blue") +
  autolayer(pred2$mean, 
            color = "blue",
            linetype = "dashed") +
  autolayer(pred3$mean, 
            color = "red") +
  autolayer(pred4$mean, 
            color = "red", 
            linetype = "dashed")

```

Solid lines are have no $\phi$ parameter, while dashed lines have $\phi$ = 0.85. The blue lines are additive models and the red lines are multiplicative models.


# Where to go Next

* Advanced time series models
  + ARCH, GARCH, etc. that model changing variance over time
* Vector Autoregression (VAR)
  + For multivariate i.e. multiple time series and modeling dependencies between them
* Machine Learning
  + How to do CV with time series
  + Neural networks for sequence data (LSTMs, etc.)
* Spatial Statistics
  + Generalize time dependence to spatial dependence in multiple dimensions
* Econometrics
  + Cointegration
  + Granger Causality
  + Serial correlation
  + Regression with time series data
* Bayesian time series


# Session Information

```{r Session_Info, echo = F, comment = ""}

# Add session information to help with reproduceability
sessionInfo()


```


